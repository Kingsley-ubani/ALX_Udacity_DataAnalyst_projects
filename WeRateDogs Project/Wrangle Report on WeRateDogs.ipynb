{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8009e84d",
   "metadata": {},
   "source": [
    "\n",
    "# Wrangling Report\n",
    "\n",
    "This report\tdescribes the wrangling process involved in completing the WeRateDogs Project.\n",
    "\n",
    "The\tData Wrangling process contained:\n",
    "\n",
    "* Data Gathering\n",
    "* Assessing Data\n",
    "* Cleaning Data\n",
    "\n",
    "\n",
    "### Data Gathering\n",
    "\n",
    "The Data Gathering stage encompasses obtaining three different datasets from three sources. I got the first data-set by downloading csv twitter_archive file manually.For the second, I had to download a tsv file programmatically using Python Requests library. The file contained image predictions of various dog breed sourcing from a neural network on some of the tweets already downloaded in the\ttwitter_archive\tfile. The third and final dataset was obtained using a Python library called Tweepy. The data obtained here was in a json format and contained more data on the tweets from the twitter_archive.\n",
    "\n",
    "\n",
    "### Assessing Data\t\n",
    "\n",
    "Assessing Data describes inspecting the data visually and programmatically. Visual inspection basically involves observing the structure of the data which is a major set-back when dealing with big data. In the course of this project, I focused more in the programmtical inspection. I was able to execute this by checking the info, shape, head, sample and value_counts of the dataframes. However,the datasets were accessed under two criterias, Quality and Tidiness.\n",
    "\n",
    "Quality involves issues\trelating to\tthe\tcontent of\tthe\tdata. This includes completeness, validity, accuracy and consistency of a data-set. I was able to identify missing records,incorrect data-type and incorrect data-entry.\n",
    "\n",
    "Tidiness invloves issues relating to the structure of the data. In making sure the data was tidy, I corrected the incorrect columns in these data-sets and ensured these columns are descriptive enough for comprehension and analysis.\n",
    "\n",
    "\n",
    "### Cleaning Data\n",
    "\n",
    "This is regarded as the final step of the Data Wrangling Process. It basically entails providing solutions to the Quality and Tidiness issues stated in the Assessing Data stage. It contains the Define, Code and Test.  \n",
    "\n",
    "In providing solution to these issues, I created a copy of these files and programmatically ensured necessary python packages and libraries were used effectively. I defined the solutions to the various quality and tidiness issues, wrote out the codes and tested by checking to see the solutions were properly implemented. \n",
    "\n",
    "Finally, I merged the 3 cleaned Dataframes and assigned it as 'twitter_archive_master'. This was further used to generate quality insights,deeper analysis and visualization.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
